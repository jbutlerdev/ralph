# Ralph Wiggum: The AI Loop Coding Technique

## Overview

Ralph Wiggum is an AI-driven development methodology that uses autonomous LLM agents in a continuous loop to implement software with minimal human intervention. Named after The Simpsons character known for being simple and persistent, the technique embodies the philosophy: keep running the same prompt over and over until the task is complete.

## Core Concept

The technique transforms AI from a pair programmer into a relentless worker that doesn't stop until the job is done. Rather than attempting a single perfect session, you run the same AI agent repeatedly on one prompt. Each iteration sees previous work via git history and file modifications, allowing continuous improvement through iteration.

### The Basic Loop

```bash
while :; do cat PROMPT.md | claude ; done
```

This continuously sends prompt instructions to an AI coding agent without limiting tool calls or usage.

## How It Works

### Loop Mechanism

1. Claude works on the assigned task
2. Attempts to exit upon completion
3. A stop hook blocks the exit (exit code 2)
4. The same prompt is fed back to Claude
5. This repeats until a specific completion signal is detected

### Three-Phase Architecture

**Phase 1: Requirements Definition**
- Human and LLM collaborate to identify Jobs-to-be-Done (JTBD)
- Break JTBDs into "topics of concern"
- Create specification files: `specs/[topic].md`

**Phase 2: Planning Mode**
- AI performs gap analysis comparing specs against existing code
- Generates prioritized task list in `IMPLEMENTATION_PLAN.md`
- Uses subagents to study codebase and identify missing features

**Phase 3: Building Mode**
- AI selects most important task from plan
- Implements, tests, and commits changes
- Updates plan and repeats until complete

## Installation & Usage

### Claude Code Plugin

```bash
# Install the plugin
/plugin install ralph-loop@claude-plugins-official

# Start a loop
/ralph-loop "Add JSDoc comments to all exported functions in src/utils/" --max-iterations 10

# Cancel the active loop
/cancel-ralph
```

### Key Options

- `--max-iterations <n>` - Safety net to prevent infinite loops (default: unlimited)
- `--completion-promise "<text>"` - Exact phrase that signals completion (e.g., "DONE", "COMPLETE")

Windows users: Install `jq` dependency first or use WSL.

## Key Principles

### 1. Iterative Tuning
Ralph requires continuous refinement. When the technique produces undesirable results, operators add "signs" (additional instructions) to guide future iterations. Over time, Ralph learns from these corrections.

### 2. Eventual Consistency
Building with Ralph demands faith in eventual consistency. The technique will test operators by taking wrong directions, requiring patience and introspection rather than blaming the tools.

### 3. Context Is Everything
Each iteration gets a fresh context window. Subagents extend memory capacity (~156kb each). Keep prompts brief and deterministic.

### 4. Steering Through Backpressure
Tests, type checks, and lints reject invalid work. The AI must fix all failures before committing. This provides quality signals and prevents broken code from compounding.

### 5. Let Ralph Ralph
Trust the AI to self-correct through iteration. The plan is disposable—regenerate if wrong. "Eventual consistency achieved through iteration."

### 6. Run Protected
Always use `--dangerously-skip-permissions` flag. Run in sandboxed environments (Docker, cloud containers). Never expose private credentials.

## Critical Files

- `loop.sh` - Orchestrates the automated iterations
- `PROMPT_plan.md` - Planning mode instructions
- `PROMPT_build.md` - Building mode instructions
- `AGENTS.md` - Project-specific operational commands (build, test, etc.)
- `IMPLEMENTATION_PLAN.md` - Living task list generated by AI
- `specs/*.md` - Requirements documents
- `progress.txt` - Log of completed work (prevents repeating mistakes)
- `src/` and `src/lib/` - Application code and shared utilities

## Operational Techniques

### Task Selection
The implementation plan persists between iterations as "shared state." Each fresh agent instance reads the updated plan and selects the next most important task automatically.

### Scope Discipline
- One task per loop iteration
- Commit only when tests pass
- No placeholders or stubs—implement completely

### Deterministic Setup
Every iteration loads identical base files (PROMPT.md + AGENTS.md), ensuring predictable starting state regardless of iteration count.

### Progress Tracking
Agent commits work each iteration and appends to `progress.txt`. This serves as a log for future iterations and prevents repeating mistakes.

## Best Practices

### DO

- Start small with 10-20 iterations to understand token consumption
- Use specific, objective completion criteria
- Keep CI green—each iteration must pass tests
- Use tests/build success as completion criteria
- Monitor usage during longer runs
- Review git diff when complete
- Set conservative `--max-iterations` initially
- Scale up after understanding consumption patterns

### DON'T

- Be too ambitious on first run
- Use vague completion criteria like "make it better"
- Forget to keep tests passing
- Use for judgment-heavy tasks like architectural decisions
- Apply to security-sensitive code requiring human review
- Use for exploration tasks without clear endpoints
- Apply to production code without thorough review

## Ideal Use Cases

Tasks with clear completion criteria and mechanical execution:

- Large refactors (e.g., class components to hooks)
- Framework migrations (e.g., Jest to Vitest)
- TDD workflows (implement features to make tests pass)
- Test coverage improvements
- TypeScript type annotation additions
- Greenfield builds with defined specs
- Adding JSDoc comments across codebases
- Automated overnight/weekend development

## Poor Use Cases

- Ambiguous requirements where "done" can't be defined precisely
- Architectural decisions needing human reasoning
- Security-sensitive code (auth, payments, data handling)
- Exploratory debugging tasks
- Tasks requiring aesthetic judgment
- Production system debugging

## Cost Awareness

A 50-iteration loop can cost $50-100+ depending on context size.

**Cost Management Strategies:**
- Set conservative `--max-iterations` initially
- Start with smaller codebases
- Use more specific criteria to reduce iterations
- Monitor usage during runs
- Consider offline/batch processing for large tasks

## Enhanced Practices

### Work Branches
Create scoped implementation plans for feature branches using `./loop.sh plan-work "description"`

### Acceptance-Driven Testing
Derive test requirements directly from acceptance criteria during planning phase

### Non-Deterministic Backpressure
Use LLM-as-judge for subjective quality evaluation (tone, aesthetics, UX)

### SLC Releases
Structure activities as user journeys and deliver Simple, Lovable, Complete horizontal slices rather than MVPs

## Troubleshooting

### Loop Stuck in Infinite Cycle
- Check completion promise achievability
- Lower iteration limit
- Review `progress.txt` for patterns

### Tests Keep Failing
- Prompt asking too much, break into smaller chunks
- Check for missing dependencies
- Review AGENTS.md commands

### Costs Too High
- Reduce iteration count
- Start with smaller codebases
- Use more specific criteria

### Agent Claims "Done" But Isn't
- Completion promise too vague
- Add objective criteria (tests pass, build succeeds)
- Review implementation plan for missing tasks

## Philosophy Shift

Move from "one-shot perfection" to "iteration over perfection." The skill shifts from directing step-by-step to writing prompts that converge toward correct solutions. Design conditions where iteration leads to success.

Humans "sit on the loop" rather than "in the loop"—providing occasional course correction rather than constant direction.

## Real-World Results

- Y Combinator hackathon: Delivered six repositories overnight
- Contract work: One engineer delivered a $50,000 USD contract for $297 using Ralph
- Complete projects: Built the CURSED programming language, demonstrating the ability to create and program in languages not present in the LLM's training data

## Why It Works

The technique succeeds by treating the LLM as an autonomous developer that learns from:

1. Explicit instructions in prompts
2. Implicit patterns in existing code
3. Operational learnings captured in AGENTS.md
4. Test failures as quality signals
5. Git history showing what has been tried

The loop provides continual course correction through observation and iteration, making operator skill more important than model quality.

## Summary

Ralph Wiggum is a development methodology built around continuous AI agent loops. At its core, it's elegantly simple: keep feeding an AI agent a task until the job is done. The technique emphasizes that success depends on well-designed prompts, clear completion criteria, and the patience to let iteration drive improvement. It can replace most outsourcing for greenfield projects and has demonstrated significant cost savings in real-world applications.
